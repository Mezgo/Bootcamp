![HenryLogo](https://d31uz8lwfmyn8g.cloudfront.net/Assets/logo-henry-white-lg.png)

# **Aprendizaje no supervisado**

Lleg√≥ el momento de dejar atr√°s, al menos por un rato, al **aprendizaje supervisado**. Ahora nos abocaremos al estudio del **aprendizaje no supervisado**.

En esta etapa cambiaremos el enfoque con el que ven√≠amos trabajando hasta el momento. `No supervisado` significa que ya no contamos con las etiquetas asociadas a nuestros datos. Es decir que las instancias no tienen asignadas una clase o un valor de salida. 

Muchas veces se indica que esta subrama de Machine Learning se aplica para encontrar patrones.

Pues esto es as√≠. El objetivo aqu√≠ ya no consiste en predecir la etiqueta, sino -precisamente- en encontrar patrones en el set de datos.

Veremos dos t√©cnicas de aprendizaje no supervisado. A saber:

+ Clustering

+ Reducci√≥n de dimensionalidad

- - -

## ***Clustering***

T√©cnica para agrupar datos de acuerdo a cu√°nto se parecen entre s√≠. Dado un set de datos, nuestra meta ser√° encontrar grupos -clusters- en los cuales las instancias pertenecientes sean parecidas -est√©n cerca-.

<img src = "../_src/assets/clust.png" height = 350>

**Pero ¬øc√≥mo hacemos para agrupar los datos?**

<img src = "../_src/assets/hom.png" height = 200>

üö® Spoiler Alert: como no pod√≠a ser de otra forma, existen diversos algoritmos que nos facilitan esta tarea. Ellos son:

+ K-means

+ DBSCAN

+ Hierarchical clustering

+ Fuzzy C-means

+ GMM

El alcance de este curso cubrir√° los dos primeros.

### `K-means`

El fundamento de este algoritmo radica en separar los datos en K clusters, ubicando a las instancias que est√©n dentro de una regi√≥n cercana de un mismo cluster.

<img src = "../_src/assets/kmeans.png" height = 150>

El centro de cada cluster -centroide- es el promedio de todos los puntos pertenecientes a ese cluster.

Cada punto est√° m√°s cerca del centro de su cluster que de cualquier otro centroide.

Trabaja de manera iterativa:

1) Se inicializan los K centroides

2) Se asigna cada instancia al centroide m√°s cercano

3) Se actualizan los centroides -la nueva posici√≥n del centroide es el promedio de las posiciones de las instancias en ese cluster (means)-

4) Se repiten los pasos 2 y 3 -hasta que la posici√≥n del centroide no var√≠e- 

Recomendamos el siguiente [enlace](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) de la Stanford University sobre el funcionamiento de este algoritmo.

### `DBSCAN`

*Density-Based Spatial Clustering of Applications with Noise*.

<img src = "https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/53842/versions/4/screenshot.png" height = 250 >

Quiz√°s es un poco menos utilizado que el anterior, pero no por ello menos importante. No es necesario aqu√≠ seleccionar la cantidad de clusters, ya que el modelo los definir√° autom√°ticamente a partir de la densidad de puntos y no de centroides. Tambi√©n incorpora la dimensi√≥n de outliers -ruido-, los cuales deja sin clasificar.

El modelo permite agrupar formas m√°s complejas, no necesariamente globulares.

Los cluster son regiones densas en el espacio de datos. Cada punto del cluster tiene que tener un m√≠nimo de vecinos en un radio determinado para no ser un outlier.

Hay tres tipos de puntos:

+ Core point

+ Border point

+ Noise

<img src = "https://miro.medium.com/max/627/1*yT96veo7Zb5QeswV7Vr7YQ.png" height = 250>

Hiperpar√°metros claves del algoritmo:

+ Epsilon: magnitud del radio

+ MinPoints: cantidad m√≠nima de vecinos

Para profundizar en este algoritmo, este [link](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html) ser√° de gran ayuda.

Respecto a la elecci√≥n de uno u otro, depender√° del tipo de datos con el que contemos. Podemos se√±alar que `K-means` se adec√∫a mejor a clusters alejados, bien agrupados y globulares. Por su parte, DBSCAN es m√°s flexible y permite adaptarse a formas m√°s complejas.
- - -

[K-Means Clustering] (https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)
[DBSCAN Clustering] (https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)

Cuadro comparativo K-Means y DBSCAN:

| K-Means | DBSCAN  |
| :------ | -----:  |
| Muy R√°pido  | Es computacionalmente m√°s costoso |
| No tiene par√°metros | Hay que elegir bien los par√°metros  |
| F√°cil de asignar nuevas instancias  | |
| Hay que definir el n√∫mero de clusters | No hay que elegir el n√∫mero de clusters |
| S√≥lo funciona bien con clusters tipo esferas  | Detecta cualquier forma de clusters |
| Sensible a outliers | Determina autom√°ticamente los outliers  |
| | No anda bien si hay clusters de diferentes densidades |

## **M√©tricas de evaluaci√≥n**

Veremos, ahora, las m√©tricas existentes para evaluar los modelos de aprendizaje no supervisado. A diferencia del paradigma del aprendizaje supervisado, que estuvimos viendo hasta hoy, aqu√≠ no tenemos etiquetas para comparar cu√°n alejado de ellas estuvo nuestro valor predicho. 

Pasemos a ver qu√© opciones tenemos para evaluar nuestros modelos en el aprendizaje no supervisado.

### `Elbow`

Se emplea para el algoritmo K-means. Busca la mejor cantidad K de clusters. Para ello, mide la distancia de cada punto al centroide m√°s cercano. Luego de entrenar el modelo, la variable `model.inertia` tiene esta informaci√≥n.

<img src = "https://i.imgur.com/DMXslDR.png" height = 300>

K √≥ptimo => buscar d√≥nde est√° el codo de la curva. El valor de inercia siempre desciende con el n√∫mero de clusters.

Si utilizamos como K los valores que est√°n a la derecha del K √≥ptimo, nuestro modelo comenzar√° a overfittear.

### `Silhouette`

Con esta m√©trica medimos qu√© tan parecidos son los datos con su propio cluster -cohesi√≥n- en comparaci√≥n con qu√© tan parecidos son a otros clusters -separaci√≥n-.

<img src = "../_src/assets/silo.png" height = 80>

Se utiliza para cualquier t√©cnica de clustering.

Su valor oscila entre -1 y 1. Un valor alto indica que el objeto est√° bien emparejado con su propio cluster y mal emparejado con los clusters vecinos. Si la mayor√≠a de los objetos tienen un valor alto, entonces la configuraci√≥n del cluster es apropiada. Si muchos puntos tienen un valor bajo o negativo, entonces la configuraci√≥n de cluster puede tener demasiados o muy pocos clusters.

<img src = "https://bookdown.org/dparedesi/data-science-con-r/Data-Science-con-R_files/figure-html/unnamed-chunk-654-1.png" height = 300>

- - -

## ***Reducci√≥n de dimensionalidad***

La reducci√≥n de dimensionalidad busca disminuir la cantidad de features de un dataset, pero reteniendo la mayor cantidad de informaci√≥n posible.

Esto sirve para:

+ Mejorar eficiencia en modelos de regresi√≥n y clasificaci√≥n

+ Disminuir el ruido

+ Facilitar la visualizaci√≥n

+ Detectar features relevantes en datasets

Esta t√©cnica del aprendizaje no supervisado, generalmente, forma parte de la etapa de `preprocesamiento de datos`. Es decir, aplicamos primero este modelo previo a la utilizaci√≥n de otro modelo de Machine Learning.

Algoritmos utilizados para reducci√≥n de dimensionalidad:

+ SVD

+ PCA

+ MDS

+ t-SNE

+ Auto-encoders

+ LDA

Nuevamente, veremos los dos primeros algoritmos que son los m√°s utilizados en la materia.

### `SVD`

*Singular Value Decomposition*.

Es un m√©todo de √°lgebra lineal que nos permite representar cualquier matriz en t√©rminos de la multiplicaci√≥n de otras tres matrices.

<img src = "../_src/assets/svd.jpg" height = 300>

Entre sus tantas utilidades, podemos mencionar la de **reducir** una matriz M -pasar de tener muchos features a tener menos, pero que sean buenos-.

El objetivo consiste en reducir la cantidad de features. Para lograrlo, buscamos crear una nueva matriz B que reemplace a la M, para que tenga menos columnas -es decir, menos atributos-. Esto se conoce como **SVD truncado**.

<img src = "../_src/assets/svd_2.jpg" height = 300>

El hiperpar√°metro que debemos establecer en este modelo es **r**, que representa cu√°ntos features terminaremos teniendo.

### `PCA`

*Principal Component Analysis*.

El m√©todo de PCA nos permite reducir la dimensionalidad de un conjunto de datos, sin perder informaci√≥n esencial. Funciona transformando un conjunto de $d$ variables correlacionadas $X=[X_1,X_2,...,X_d]$ en un conjunto de variables no correlacionadas $W=[W_1,W_2,...,W_d]$.

A trav√©s de combinaciones lineales de las variables originales que maximizan la varianza explicada se consiguen los llamados componentes principales. Estos son ortogonales entre si (producto escalar 0).

PCA se basa en el teorema de descomposici√≥n espectral de la llamada matriz de varianzas y covarianzas $\Sigma_{d\times d}$

La primera componente principal est√° en la direcci√≥n donde los datos presentan varianza m√°xima. La segunda componente principal est√° en la segunda direcci√≥n en t√©rminos de varianza, y as√≠ sucesivamente.

Estas componentes pueden representar dimensiones no medibles/no medidas en nuestros datos pero que est√°n altamente correlacionadas: si poseemos, por ejemplo, una curva de bonos, teniendo **d** maturities (duraci√≥n del bono) diferentes, podemos conseguir **d** componentes principales. Una vez que conseguimos, observando el comportamiento de los primeros tres, podemos explicar cerca del 100% de la variabilidad de los datos y podemos dar interpretaciones a cada componente.

El hiperpar√°metro principal a establecer es la cantidad de variables con las que nos queremos quedar.

<<<<<<< HEAD
<img src = "../_src/assets/PCA.png" height = 300>

<img src = "../_src/assets/PCA2.png" height = 250>
=======
<img src = "../_src/assets/PCA.png" height = 250>

<img src = "../_src/assets/PCA2.png" height = 300>
>>>>>>> d48132453714123eeeb2410cacea843e6177b210

Dejamos un breve [video](https://www.youtube.com/watch?v=HMOI_lkzW08&ab_channel=StatQuestwithJoshStarmer) explicativo de el algoritmo PCA.

## Sistemas de Recomendaci√≥n

Es muy com√∫n encontrar en diversas plataformas, recomendaciones de productos para consumo, en base al producto seleccionado:<br>
<img src="../_src/assets/sistemas_recomendacion1.jpg" height="300"><br>

* Existen usuarios e √≠tems. Los usuarios prefieren algunos √≠tems por sobre otros.
* Ejemplo: Usuarios de Netflix y Pel√≠culas. De 1 a 5 estrellas.
* El objetivo del sistema de recomendaci√≥n es poblar la matriz de utilidad de una manera inteligente y bajo los requisitos que imponga cada entorno.<br>
<img src="../_src/assets/sistemas_recomendacion2.jpg" height="200"><br>
* Por ejemplo, Netflix tiene 150 millones suscriptores y 5 mil pel√≠culas. La matriz tiene 750 mil millones de espacios, de los cuales la mayor√≠a est√°n vac√≠os.
* Cuando buscamos recomendar, interesa m√°s recomendar √≠tems que van a gustar que aquellos que no van a gustar.
* En algunos casos, interesa mostrar a los usuarios novedades. 
* Algunas veces, ni siquiera hay calificaciones, solamente si vio o no (o escuch√≥, ley√≥, compr√≥, etc.).
* Hist√≥ricamente, las recomendaciones se hac√≠an por medio de cr√≠tica de expertos, listas de favoritos, listas de cl√°sicos, m√°s populares, recientes, etc. Hoy las recomendaciones son espec√≠ficas para cada usuario.

### Es posible diferenciar dos formas de hacer las recomendaciones:

1) Pedir a los usuarios que punt√∫en los √≠tems.
  * Los usuarios no suelen hacerlo
  * Si lo hacen, puede estar sesgado (gente que prefiere puntuar cosas que no le gustan a puntuar cosas que s√≠, etc.).
2) Inferir a partir de acciones
  * Ejemplo: compra muchas cosas de camping ‚Üí le gusta el camping, aire libre, etc.
  * ¬øQu√© pasa con las cosas que no le gustan?

<img src="../_src/assets/sistemas_recomendacion3.jpg" height="300"><br>
<img src="../_src/assets/sistemas_recomendacion4.jpg" height="300"><br>

### Filtro basado en contenido:

1) Para cada √≠tem, debemos construir un perfil. 
    * Casos sencillos: informaci√≥n f√°cilmente disponible. Pel√≠culas: director, g√©nero, actores, a√±o, etc.
    * Casos complejos: Debemos extraer features de los √≠tems. Noticias: hay que usar la bater√≠a de herramientas de NLP (tf-idf, etc.)
2) Idealmente, tambi√©n hay que construir un perfil de qu√© cosas le gustan al usuario.
3) Usamos una m√©trica de distancia para encontrar √≠tems similares.
    * √çndice Jaccard
    * Distancia coseno
4) Recomendamos 

### Filtro colaborativo:

1) Se debe llenar la matriz de utilidad, por ejemplo con t√©cnicas de clusterizaci√≥n para encontrar grupos de usuarios similares. De esos usuarios similares, los que tengan alg√∫n faltante en un √≠tem, se lo completa con, por ejemplo, el promedio del cluster.
2) Descomposici√≥n UV:<br>
<img src="../_src/assets/sistemas_recomendacion5.jpg" height="150"><br>
<img src="../_src/assets/sistemas_recomendacion6.jpg" height="300"><br>

#### ¬øC√≥mo encontrar los valores para U y V?

* Utilizando una m√©trica para minimizar. En general, RMSE para los valores no nulos de la matriz.
* Se comienza en alg√∫n lugar al azar.
* Se busca el m√≠nimo de la funci√≥n de costo. Es el problema que resuelve el descenso por gradiente.

Un modelo h√≠brido, que utilice en paralelo ambos m√©todos, en ocasiones puede ser lo m√°s adecuado<br>
<img src="../_src/assets/sistemas_recomendacion7.jpg" height="400"><br>

- - -

## Pr√°ctica guiada

La pr√°ctica de la clase de hoy se conforma de la siguiente manera:

+ Pr√°ctica_01: modelos de clustering. Usaremos K-means y DBSCAN

+ Pr√°ctica_02: m√©tricas de evaluaci√≥n de clustering

+ Pr√°ctica_03: modelos de reducci√≥n de dimensionalidad. Usaremos SVD y PCA

+ Pr√°ctica_04: Sistemas de Recomendaci√≥n

Para realizar estas pr√°cticas, en la carpeta Datasets poseen un .zip que contiene los archivos a utilizar.

- - -

**La clase que viene volveremos al aprendizaje supervisado.**

<img src = "https://www.pintzap.com/storage/img/memegenerator/templates/homer-simpson-yuju.jpg" height = 200>
